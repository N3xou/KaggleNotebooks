{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34377,"databundleVersionId":3220602,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow_decision_forests as tfdf\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:34.222804Z","iopub.execute_input":"2025-03-28T22:28:34.223341Z","iopub.status.idle":"2025-03-28T22:28:38.393144Z","shell.execute_reply.started":"2025-03-28T22:28:34.223284Z","shell.execute_reply":"2025-03-28T22:28:38.391863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = pd.read_csv(\"/kaggle/input/spaceship-titanic/train.csv\")\ndataset = dataset.map(lambda x: int(x) if isinstance(x,bool) else x)\n\ntf_dataset = tfdf.keras.pd_dataframe_to_tf_dataset(dataset, label=\"Transported\")\n\nmodel = tfdf.keras.RandomForestModel()\nmodel.fit(tf_dataset)\n\n#print(model.summary())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:38.394791Z","iopub.execute_input":"2025-03-28T22:28:38.395787Z","iopub.status.idle":"2025-03-28T22:28:48.764592Z","shell.execute_reply.started":"2025-03-28T22:28:38.395743Z","shell.execute_reply":"2025-03-28T22:28:48.763447Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_decision_forests as tfdf\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:48.767088Z","iopub.execute_input":"2025-03-28T22:28:48.767483Z","iopub.status.idle":"2025-03-28T22:28:49.408372Z","shell.execute_reply.started":"2025-03-28T22:28:48.767450Z","shell.execute_reply":"2025-03-28T22:28:49.407416Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EXPLORATION\n","metadata":{}},{"cell_type":"code","source":"dataset_df = pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')\nprint(\"Full train dataset shape is {}\".format(dataset_df.shape))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:49.409914Z","iopub.execute_input":"2025-03-28T22:28:49.410806Z","iopub.status.idle":"2025-03-28T22:28:49.442078Z","shell.execute_reply.started":"2025-03-28T22:28:49.410761Z","shell.execute_reply":"2025-03-28T22:28:49.440880Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#dataset_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:49.443089Z","iopub.execute_input":"2025-03-28T22:28:49.443408Z","iopub.status.idle":"2025-03-28T22:28:49.447636Z","shell.execute_reply.started":"2025-03-28T22:28:49.443354Z","shell.execute_reply":"2025-03-28T22:28:49.446406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#print(dataset_df.describe())\n#dataset_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:49.448712Z","iopub.execute_input":"2025-03-28T22:28:49.449017Z","iopub.status.idle":"2025-03-28T22:28:49.468161Z","shell.execute_reply.started":"2025-03-28T22:28:49.448989Z","shell.execute_reply":"2025-03-28T22:28:49.466686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_df = dataset_df.drop(['PassengerId', 'Name'], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:49.469761Z","iopub.execute_input":"2025-03-28T22:28:49.470156Z","iopub.status.idle":"2025-03-28T22:28:49.490738Z","shell.execute_reply.started":"2025-03-28T22:28:49.470113Z","shell.execute_reply":"2025-03-28T22:28:49.489523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#dataset_df[['VIP', 'CryoSleep', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']] = dataset_df[['VIP', 'CryoSleep', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].fillna(value=0)\ndataset_df.isnull().sum().sort_values(ascending=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:49.494213Z","iopub.execute_input":"2025-03-28T22:28:49.494635Z","iopub.status.idle":"2025-03-28T22:28:49.516650Z","shell.execute_reply.started":"2025-03-28T22:28:49.494602Z","shell.execute_reply":"2025-03-28T22:28:49.515487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_df = dataset_df.map(lambda x: int(x) if isinstance(x,bool) else x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:49.518885Z","iopub.execute_input":"2025-03-28T22:28:49.519257Z","iopub.status.idle":"2025-03-28T22:28:49.571456Z","shell.execute_reply.started":"2025-03-28T22:28:49.519215Z","shell.execute_reply":"2025-03-28T22:28:49.570193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_df[[\"Deck\", \"Cabin_num\", \"Side\"]] = dataset_df[\"Cabin\"].str.split(\"/\", expand=True)\ntry:\n    dataset_df = dataset_df.drop('Cabin', axis=1)\nexcept KeyError:\n    print(\"Field does not exist\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:49.572594Z","iopub.execute_input":"2025-03-28T22:28:49.572946Z","iopub.status.idle":"2025-03-28T22:28:49.603467Z","shell.execute_reply.started":"2025-03-28T22:28:49.572914Z","shell.execute_reply":"2025-03-28T22:28:49.602249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#dataset_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:49.604567Z","iopub.execute_input":"2025-03-28T22:28:49.604961Z","iopub.status.idle":"2025-03-28T22:28:49.620760Z","shell.execute_reply.started":"2025-03-28T22:28:49.604924Z","shell.execute_reply":"2025-03-28T22:28:49.619526Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# More visualisation","metadata":{}},{"cell_type":"code","source":"# 1. Correlation Heatmap for numerical features\nplt.figure(figsize=(10,6))\nnum_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"Transported\"]\nsns.heatmap(dataset_df[num_cols].corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\nplt.title(\"Feature Correlation Heatmap\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:49.621744Z","iopub.execute_input":"2025-03-28T22:28:49.622047Z","iopub.status.idle":"2025-03-28T22:28:50.024888Z","shell.execute_reply.started":"2025-03-28T22:28:49.622018Z","shell.execute_reply":"2025-03-28T22:28:50.023748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Feature Distributions by Target\nfig, axes = plt.subplots(2, 3, figsize=(15,10))\nnum_features = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\nfor i, feature in enumerate(num_features):\n    sns.histplot(data=dataset_df, x=feature, hue=\"Transported\", element=\"step\", kde=True, ax=axes[i//3, i%3])\n    axes[i//3, i%3].set_title(f\"{feature} Distribution by Transported\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:50.026135Z","iopub.execute_input":"2025-03-28T22:28:50.026476Z","iopub.status.idle":"2025-03-28T22:28:52.169953Z","shell.execute_reply.started":"2025-03-28T22:28:50.026445Z","shell.execute_reply":"2025-03-28T22:28:52.168371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Categorical Feature Impact\nfig, axes = plt.subplots(1, 3, figsize=(18,5))\ncategories = [\"HomePlanet\", \"CryoSleep\", \"Destination\"]\nfor i, cat in enumerate(categories):\n    sns.countplot(data=dataset_df, x=cat, hue=\"Transported\", ax=axes[i], palette=[\"#1f77b4\", \"#ff7f0e\"])\n    axes[i].set_title(f\"{cat} vs Transported\")\n    axes[i].tick_params(axis='x', rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:52.171370Z","iopub.execute_input":"2025-03-28T22:28:52.171830Z","iopub.status.idle":"2025-03-28T22:28:52.844968Z","shell.execute_reply.started":"2025-03-28T22:28:52.171786Z","shell.execute_reply":"2025-03-28T22:28:52.843638Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Working on features\n","metadata":{}},{"cell_type":"markdown","source":"Replace NaN with median (for numerical) and most frequent (for categorical)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Copy the dataset\ncleaned_df = dataset_df.copy()\n\n# List of numerical and categorical columns\nnumerical_columns = ['RoomService', 'Age', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Cabin_num']\ncategorical_columns = ['HomePlanet', 'CryoSleep', 'Deck', 'Side', 'Destination', 'VIP', 'Name']\n\n# Check if 'Name' column exists in the DataFrame, and remove it from categorical_columns if it's missing\nif 'Name' not in cleaned_df.columns:\n    categorical_columns.remove('Name')\n\n# Handle potential non-numeric issues in 'Cabin_num' by converting it to numeric\ncleaned_df['Cabin_num'] = pd.to_numeric(cleaned_df['Cabin_num'], errors='coerce')\n\n# Fill numerical columns with median (direct assignment)\nfor col in numerical_columns:\n    cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].median())\n\n# Fill categorical columns with the most frequent value (direct assignment)\nfor col in categorical_columns:\n    cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].mode()[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:52.845794Z","iopub.execute_input":"2025-03-28T22:28:52.846156Z","iopub.status.idle":"2025-03-28T22:28:52.875708Z","shell.execute_reply.started":"2025-03-28T22:28:52.846124Z","shell.execute_reply":"2025-03-28T22:28:52.874516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bins = [0, 5, 10, 18, 25, 35, 45, 60, 100]\nlabels = ['0-5', '6-10', '11-18', '19-25', '26-35', '36-45', '46-60', '60+']\n\n# Create a new feature 'Age_Group' based on the bins\ncleaned_df['Age_Group'] = pd.cut(cleaned_df['Age'], bins=bins, labels=labels, right=False)\n\n\ncleaned_df[\"TotalSpending\"] = (\n    cleaned_df[\"Spa\"] + cleaned_df[\"VRDeck\"] + \n    cleaned_df[\"RoomService\"] + cleaned_df[\"ShoppingMall\"] + \n    cleaned_df[\"FoodCourt\"]\n)\n\ncleaned_df[\"LuxuryUser\"] = (cleaned_df[\"Spa\"] > 0) | (cleaned_df[\"VRDeck\"] > 0) | (cleaned_df[\"RoomService\"] > 0)\ncleaned_df[\"SpendingPerAge\"] = cleaned_df[\"TotalSpending\"] / (cleaned_df[\"Age\"] + 1)\n\ncleaned_df[\"HighSpender\"] = cleaned_df[\"TotalSpending\"] > cleaned_df[\"TotalSpending\"].median()\n\ncleaned_df = cleaned_df.drop(columns = [\"Age\"])\ncleaned_df = pd.get_dummies(cleaned_df, columns=['Age_Group'], drop_first=False)\n\nprint(cleaned_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:52.876799Z","iopub.execute_input":"2025-03-28T22:28:52.877196Z","iopub.status.idle":"2025-03-28T22:28:52.918808Z","shell.execute_reply.started":"2025-03-28T22:28:52.877159Z","shell.execute_reply":"2025-03-28T22:28:52.917452Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split dataset (80% train, 20% test by default)\n\n\ncleaned_df = cleaned_df.map(lambda x: int(x) if isinstance(x,bool) else x)\ntrain_ds_pd, valid_ds_pd = train_test_split(cleaned_df, test_size=0.3, random_state=42)\nprint(cleaned_df.head())\n# Print dataset sizes\n#print(f\"{len(train_ds_pd)} examples in training, {len(valid_ds_pd)} examples in testing.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:52.919796Z","iopub.execute_input":"2025-03-28T22:28:52.920083Z","iopub.status.idle":"2025-03-28T22:28:53.085634Z","shell.execute_reply.started":"2025-03-28T22:28:52.920048Z","shell.execute_reply":"2025-03-28T22:28:53.084299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(type(train_ds_pd))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:53.086737Z","iopub.execute_input":"2025-03-28T22:28:53.087117Z","iopub.status.idle":"2025-03-28T22:28:53.092607Z","shell.execute_reply.started":"2025-03-28T22:28:53.087084Z","shell.execute_reply":"2025-03-28T22:28:53.091246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tfdf.keras.get_all_models()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:53.093746Z","iopub.execute_input":"2025-03-28T22:28:53.094042Z","iopub.status.idle":"2025-03-28T22:28:53.114695Z","shell.execute_reply.started":"2025-03-28T22:28:53.094014Z","shell.execute_reply":"2025-03-28T22:28:53.113547Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Random Tree Forest","metadata":{}},{"cell_type":"markdown","source":"\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=\"Transported\")\nvalid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label=\"Transported\")\n\nrf = tfdf.keras.RandomForestModel(hyperparameter_template=\"benchmark_rank1\")\nrf.compile(metrics=[\"accuracy\"])\nrf.fit(x=train_ds)\ninspector = rf.make_inspector()\nprint(inspector.evaluation())\nevaluation = rf.evaluate(x=valid_ds,return_dict=True)\n\nfor name, value in evaluation.items():\n  print(f\"{name}: {value:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-03-28T21:49:21.065310Z","iopub.execute_input":"2025-03-28T21:49:21.065690Z","iopub.status.idle":"2025-03-28T21:49:32.505543Z","shell.execute_reply.started":"2025-03-28T21:49:21.065660Z","shell.execute_reply":"2025-03-28T21:49:32.504193Z"}}},{"cell_type":"code","source":"import tensorflow_decision_forests as tfdf\n\n\n\n# Convert data to TensorFlow dataset\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=\"Transported\")\nvalid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label=\"Transported\")\n\n\n# Initialize Gradient Boosted Trees model with tuned hyperparameters\ngbm = tfdf.keras.GradientBoostedTreesModel(\n    num_trees=300,                # More trees for better learning\n    max_depth=8,                  # Deeper trees capture more complexity\n    shrinkage=0.05,               # Lower learning rate for better convergence\n    subsample=0.8,                # Random subsampling to reduce overfitting\n)\n\ngbm.compile(metrics=[\"accuracy\"])\n\n# Train the model\ngbm.fit(x=train_ds)\n\n# Inspect the trained model\ninspector = gbm.make_inspector()\nprint(inspector.evaluation())\n\n# Evaluate the model on validation data\nevaluation = gbm.evaluate(x=valid_ds, return_dict=True)\nfor name, value in evaluation.items():\n    print(f\"{name}: {value:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2025-03-28T22:28:53.115855Z","iopub.execute_input":"2025-03-28T22:28:53.116230Z","iopub.status.idle":"2025-03-28T22:28:59.429000Z","shell.execute_reply.started":"2025-03-28T22:28:53.116199Z","shell.execute_reply":"2025-03-28T22:28:59.427734Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=\"Transported\")\nvalid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label=\"Transported\")\nrf = RandomForestClassifier(n_estimators=200, max_depth=8)\nxgb = XGBClassifier(n_estimators=300, learning_rate=0.05)\nlr = LogisticRegression()\n\nensemble_model = VotingClassifier(estimators=[\n    ('rf', rf),\n    ('xgb', xgb),\n    ('lr', lr)\n], voting='soft')\n\nensemble_model.fit(train_ds_pd.drop(columns=[\"Transported\"]), train_ds_pd[\"Transported\"])","metadata":{}},{"cell_type":"code","source":"print(f\"Available variable importances:\")\nfor importance in inspector.variable_importances().keys():\n  print(\"\\t\", importance)\ninspector.variable_importances()[\"NUM_AS_ROOT\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:59.430284Z","iopub.execute_input":"2025-03-28T22:28:59.430670Z","iopub.status.idle":"2025-03-28T22:28:59.442161Z","shell.execute_reply.started":"2025-03-28T22:28:59.430632Z","shell.execute_reply":"2025-03-28T22:28:59.441021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#tfdf.model_plotter.plot_model_in_colab(rf, tree_idx=0, max_depth=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:59.443295Z","iopub.execute_input":"2025-03-28T22:28:59.443730Z","iopub.status.idle":"2025-03-28T22:28:59.460115Z","shell.execute_reply.started":"2025-03-28T22:28:59.443696Z","shell.execute_reply":"2025-03-28T22:28:59.459021Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow_decision_forests as tfdf\n\n# Load the test dataset\ntest_df = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')\nsubmission_id = test_df[\"PassengerId\"]\n\n# Fill missing values\ntest_df[['VIP', 'CryoSleep']] = test_df[['VIP', 'CryoSleep']].fillna(value=0)\n\n# Create new features from Cabin\ntest_df[[\"Deck\", \"Cabin_num\", \"Side\"]] = test_df[\"Cabin\"].str.split(\"/\", expand=True)\ntest_df.drop(columns=['Cabin'], inplace=True)\n\n# Convert boolean features to 0s and 1s\ntest_df['VIP'] = test_df['VIP'].astype(int)\ntest_df['CryoSleep'] = test_df['CryoSleep'].astype(int)\n\nbins = [0, 5, 10, 18, 25, 35, 45, 60, 100]\nlabels = ['0-5', '6-10', '11-18', '19-25', '26-35', '36-45', '46-60', '60+']\n\n# Create a new feature 'Age_Group' based on the bins\ntest_df['Age_Group'] = pd.cut(test_df['Age'], bins=bins, labels=labels, right=False)\n\ndrop_features = [\"Age\"]\n\ntest_df['Cabin_num'] = pd.to_numeric(test_df['Cabin_num'], errors='coerce')\n\ntest_df[\"TotalSpending\"] = (\n    test_df[\"Spa\"] + test_df[\"VRDeck\"] + \n    test_df[\"RoomService\"] + test_df[\"ShoppingMall\"] + \n    test_df[\"FoodCourt\"]\n)\n\ntest_df[\"LuxuryUser\"] = (test_df[\"Spa\"] > 0) | (test_df[\"VRDeck\"] > 0) | (test_df[\"RoomService\"] > 0)\ntest_df[\"SpendingPerAge\"] = test_df[\"TotalSpending\"] / (test_df[\"Age\"] + 1)\n\ntest_df[\"HighSpender\"] = test_df[\"TotalSpending\"] > test_df[\"TotalSpending\"].median()\ntest_df = pd.get_dummies(test_df, columns=['Age_Group'], drop_first=False)\n\ntest_df = test_df.drop(columns=drop_features)\n# Convert DataFrame to TensorFlow dataset\ntest_df = test_df.map(lambda x: int(x) if isinstance(x,bool) else x)\ntest_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df)\n\n\n# Get predictions for test data\npredictions = gbm.predict(test_ds)\nn_predictions = (predictions > 0.5).astype(bool)\n\n# Create submission file\noutput = pd.DataFrame({'PassengerId': submission_id,\n                       'Transported': n_predictions.squeeze()})\n\noutput.head()\n","metadata":{"execution":{"iopub.status.busy":"2025-03-28T22:28:59.464035Z","iopub.execute_input":"2025-03-28T22:28:59.464468Z","iopub.status.idle":"2025-03-28T22:28:59.942141Z","shell.execute_reply.started":"2025-03-28T22:28:59.464422Z","shell.execute_reply":"2025-03-28T22:28:59.940674Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output.to_csv('/kaggle/working/submission.csv', index=False)\noutput.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:28:59.942926Z","iopub.status.idle":"2025-03-28T22:28:59.943282Z","shell.execute_reply":"2025-03-28T22:28:59.943143Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Extra visualisation","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nlogs = rf.make_inspector().training_logs()\nplt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\nplt.xlabel(\"Number of trees\")\nplt.ylabel(\"Accuracy (out-of-bag)\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-03-28T22:28:59.944362Z","iopub.status.idle":"2025-03-28T22:28:59.944890Z","shell.execute_reply":"2025-03-28T22:28:59.944676Z"},"trusted":true},"outputs":[],"execution_count":null}]}