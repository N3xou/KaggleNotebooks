{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34377,"databundleVersionId":3220602,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow_decision_forests as tfdf\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = pd.read_csv(\"/kaggle/input/spaceship-titanic/train.csv\")\ndataset = dataset.map(lambda x: int(x) if isinstance(x,bool) else x)\n\ntf_dataset = tfdf.keras.pd_dataframe_to_tf_dataset(dataset, label=\"Transported\")\n\nmodel = tfdf.keras.RandomForestModel()\nmodel.fit(tf_dataset)\n\n#print(model.summary())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_decision_forests as tfdf\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EXPLORATION\n","metadata":{}},{"cell_type":"code","source":"dataset_df = pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')\nprint(\"Full train dataset shape is {}\".format(dataset_df.shape))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#dataset_df.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#print(dataset_df.describe())\n#dataset_df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#dataset_df[['VIP', 'CryoSleep', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']] = dataset_df[['VIP', 'CryoSleep', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].fillna(value=0)\ndataset_df.isnull().sum().sort_values(ascending=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_df = dataset_df.map(lambda x: int(x) if isinstance(x,bool) else x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_df[[\"Deck\", \"Cabin_num\", \"Side\"]] = dataset_df[\"Cabin\"].str.split(\"/\", expand=True)\ntry:\n    dataset_df = dataset_df.drop('Cabin', axis=1)\nexcept KeyError:\n    print(\"Field does not exist\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#dataset_df.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# More visualisation","metadata":{}},{"cell_type":"code","source":"# 1. Correlation Heatmap for numerical features\nplt.figure(figsize=(10,6))\nnum_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"Transported\"]\nsns.heatmap(dataset_df[num_cols].corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\nplt.title(\"Feature Correlation Heatmap\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Feature Distributions by Target\nfig, axes = plt.subplots(2, 3, figsize=(15,10))\nnum_features = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\nfor i, feature in enumerate(num_features):\n    sns.histplot(data=dataset_df, x=feature, hue=\"Transported\", element=\"step\", kde=True, ax=axes[i//3, i%3])\n    axes[i//3, i%3].set_title(f\"{feature} Distribution by Transported\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Categorical Feature Impact\nfig, axes = plt.subplots(1, 3, figsize=(18,5))\ncategories = [\"HomePlanet\", \"CryoSleep\", \"Destination\"]\nfor i, cat in enumerate(categories):\n    sns.countplot(data=dataset_df, x=cat, hue=\"Transported\", ax=axes[i], palette=[\"#1f77b4\", \"#ff7f0e\"])\n    axes[i].set_title(f\"{cat} vs Transported\")\n    axes[i].tick_params(axis='x', rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Working on features\n","metadata":{}},{"cell_type":"markdown","source":"**Filling NaN with median (for numerical) and most frequent (for categorical)**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Copy the dataset\ncleaned_df = dataset_df.copy()\n\n# List of numerical and categorical columns\nnumerical_columns = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Cabin_num']\ncategorical_columns = ['HomePlanet', 'CryoSleep', 'Deck', 'Side', 'Destination', 'VIP', 'Name']\n\n\n\n# Handle potential non-numeric issues in 'Cabin_num' by converting it to numeric\ncleaned_df['Cabin_num'] = pd.to_numeric(cleaned_df['Cabin_num'], errors='coerce')\ncleaned_df['Age'] = cleaned_df['Age'].fillna(cleaned_df.groupby('HomePlanet')['Age'].transform('median'))\n\n# Fill numerical columns with median (direct assignment)\nfor col in numerical_columns:\n    cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].median())\n\n# Fill categorical columns with the most frequent value (direct assignment)\nfor col in categorical_columns:\n    cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].mode()[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"plt.figure(figsize=(12, 6))\nsns.violinplot(data=cleaned_df, x='HomePlanet', y='Age', hue='Transported', split=True, palette='coolwarm')\n\nplt.title(\"Age Distribution by HomePlanet & Transported\")\nplt.ylabel(\"Age\")\nplt.xlabel(\"HomePlanet\")\nplt.legend(title=\"Transported\", labels=[\"Not Transported\", \"Transported\"])\nplt.show()\n\nbins = [0, 10, 18, 25, 35, 45, 60, 100]\nlabels = ['0-10', '11-18', '19-25', '26-35', '36-45', '46-60', '60+']\ncleaned_df['Age_Bin'] = pd.cut(cleaned_df['Age'], bins=bins, labels=labels, right=False)\n\n# Calculate Transported rate per Age Bin & HomePlanet\nage_transport_rate = cleaned_df.groupby(['Age_Bin', 'HomePlanet'])['Transported'].mean().reset_index()\n\n# Plot Transported rate for each HomePlanet\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=age_transport_rate, x='Age_Bin', y='Transported', hue='HomePlanet', marker=\"o\", palette='tab10')\n\nplt.title(\"Transported Rate by Age Group & HomePlanet\")\nplt.ylabel(\"Transported Rate\")\nplt.xlabel(\"Age Group\")\nplt.xticks(rotation=45)\nplt.legend(title=\"HomePlanet\")\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-03-30T16:54:04.886683Z","iopub.execute_input":"2025-03-30T16:54:04.887053Z","iopub.status.idle":"2025-03-30T16:54:05.489746Z","shell.execute_reply.started":"2025-03-30T16:54:04.887025Z","shell.execute_reply":"2025-03-30T16:54:05.488593Z"}}},{"cell_type":"code","source":"bins = [0, 5, 10, 18, 25, 35, 45, 60, 100]\nlabels = ['0-5', '6-10', '11-18', '19-25', '26-35', '36-45', '46-60', '60+']\n\n# Create a new feature 'Age_Group' based on the bins\n\ncleaned_df[\"TotalSpending\"] = (\n    cleaned_df[\"Spa\"] + cleaned_df[\"VRDeck\"] + \n    cleaned_df[\"RoomService\"] + cleaned_df[\"ShoppingMall\"] + \n    cleaned_df[\"FoodCourt\"]\n)\n\ncleaned_df[\"LuxuryUser\"] = (cleaned_df[\"Spa\"] > 0) | (cleaned_df[\"VRDeck\"] > 0) | (cleaned_df[\"RoomService\"] > 0)\ncleaned_df[\"SpendingPerAge\"] = cleaned_df[\"TotalSpending\"] / (cleaned_df[\"Age\"] + 1)\n\ncleaned_df[\"HighSpender\"] = cleaned_df[\"TotalSpending\"] > cleaned_df[\"TotalSpending\"].median()\ncleaned_df['FamilySize'] = cleaned_df['PassengerId'].apply(lambda x: int(x.split('_')[1]))\n\ncleaned_df['Age_Group'] = pd.cut(cleaned_df['Age'], bins=bins, labels=labels, right=False)\ncleaned_df = pd.get_dummies(cleaned_df, columns=['Age_Group'], drop_first=False)\n\ncleaned_df = cleaned_df.drop(columns = [\"Age\",'PassengerId','Name'])\nprint(cleaned_df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split dataset (80% train, 20% test by default)\n\n\ncleaned_df = cleaned_df.map(lambda x: int(x) if isinstance(x,bool) else x)\ntrain_ds_pd, valid_ds_pd = train_test_split(cleaned_df, test_size=0.3, random_state=42)\nprint(cleaned_df.head())\n# Print dataset sizes\n#print(f\"{len(train_ds_pd)} examples in training, {len(valid_ds_pd)} examples in testing.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tfdf.keras.get_all_models()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Random Tree Forest","metadata":{}},{"cell_type":"markdown","source":"\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=\"Transported\")\nvalid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label=\"Transported\")\n\nrf = tfdf.keras.RandomForestModel(hyperparameter_template=\"benchmark_rank1\")\nrf.compile(metrics=[\"accuracy\"])\nrf.fit(x=train_ds)\ninspector = rf.make_inspector()\nprint(inspector.evaluation())\nevaluation = rf.evaluate(x=valid_ds,return_dict=True)\n\nfor name, value in evaluation.items():\n  print(f\"{name}: {value:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-03-28T21:49:21.065310Z","iopub.execute_input":"2025-03-28T21:49:21.065690Z","iopub.status.idle":"2025-03-28T21:49:32.505543Z","shell.execute_reply.started":"2025-03-28T21:49:21.065660Z","shell.execute_reply":"2025-03-28T21:49:32.504193Z"}}},{"cell_type":"code","source":"import tensorflow_decision_forests as tfdf\n\n\n\n# Convert data to TensorFlow dataset\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=\"Transported\")\nvalid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label=\"Transported\")\n\n\n# Initialize Gradient Boosted Trees model with tuned hyperparameters\ngbm = tfdf.keras.GradientBoostedTreesModel(\n    num_trees=400,                # More trees for better learning\n    max_depth=8,                  # Deeper trees capture more complexity\n    shrinkage=0.03,               # Lower learning rate for better convergence\n    subsample=0.7,                # Random subsampling to reduce overfitting\n)\n\ngbm.compile(metrics=[\"accuracy\"])\n\n# Train the model\ngbm.fit(x=train_ds)\n\n# Inspect the trained model\ninspector = gbm.make_inspector()\nprint(inspector.evaluation())\n\n# Evaluate the model on validation data\nevaluation = gbm.evaluate(x=valid_ds, return_dict=True)\nfor name, value in evaluation.items():\n    print(f\"{name}: {value:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow_decision_forests as tfdf\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, KFold\n\n# Assuming cleaned_df is your dataframe and is already loaded\n\n# Convert boolean columns to int\ncleaned_df = cleaned_df.map(lambda x: int(x) if isinstance(x,bool) else x)\n\n# Split the data into training and validation sets (can be done before the loop)\ntrain_ds_pd, valid_ds_pd = train_test_split(cleaned_df, test_size=0.3, random_state=42)\n\n# Convert to TensorFlow datasets\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=\"Transported\")\nvalid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label=\"Transported\")\n\n# Define the hyperparameter space\nnum_trees_values = [20, 50, 100, 300, 500]  # Different number of trees\nmax_depth_values = [4, 6, 8, 12, 16, 30]   # Different tree depths\nshrinkage_values = [0.01, 0.03, 0.05]      # Different shrinkage values\nsubsample_values = [0.4, 0.5, 0.6, 0.7]    # Different subsample rates\n\n# Create an empty list to store the evaluation results\nresults = []\n\n# Number of cross-validation folds\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Loop over all combinations of hyperparameters\nfor num_trees in num_trees_values:\n    for max_depth in max_depth_values:\n        for shrinkage in shrinkage_values:\n            for subsample in subsample_values:\n                \n                fold_accuracies = []\n                fold_losses = []\n\n                # Perform k-fold cross-validation\n                for train_index, val_index in kf.split(train_ds_pd):\n                    # Create the train and validation splits for this fold\n                    train_fold = train_ds_pd.iloc[train_index]\n                    valid_fold = train_ds_pd.iloc[val_index]\n                    \n                    # Convert to TensorFlow datasets\n                    train_fold_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_fold, label=\"Transported\")\n                    valid_fold_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_fold, label=\"Transported\")\n                    \n                    # Create and compile the model with the current hyperparameters\n                    gbm = tfdf.keras.GradientBoostedTreesModel(\n                        num_trees=num_trees,\n                        max_depth=max_depth,\n                        shrinkage=shrinkage,\n                        subsample=subsample\n                    )\n                    gbm.compile(metrics=[\"accuracy\"])\n\n                    # Print the current hyperparameters for this fold\n                    print(f\"Training with num_trees={num_trees}, max_depth={max_depth}, shrinkage={shrinkage}, subsample={subsample}\")\n\n                    # Train the model on the current fold\n                    gbm.fit(x=train_fold_ds)\n\n                    # Evaluate the model on the validation fold\n                    evaluation = gbm.evaluate(x=valid_fold_ds, return_dict=True)\n\n                    # Store the accuracy and loss for this fold\n                    fold_accuracies.append(evaluation.get(\"accuracy\", 0.0))\n                    fold_losses.append(evaluation.get(\"loss\", 0.0))\n\n                # Calculate the average accuracy and loss across all folds\n                avg_accuracy = np.mean(fold_accuracies)\n                avg_loss = np.mean(fold_losses)\n\n                # Store the results along with the hyperparameters\n                results.append({\n                    \"num_trees\": num_trees,\n                    \"max_depth\": max_depth,\n                    \"shrinkage\": shrinkage,\n                    \"subsample\": subsample,\n                    \"avg_validation_accuracy\": avg_accuracy,\n                    \"avg_validation_loss\": avg_loss,\n                })\n\n# Print the results\nfor result in results:\n    print(f\"Hyperparameters: {result['num_trees']} trees, max_depth={result['max_depth']}, shrinkage={result['shrinkage']}, subsample={result['subsample']}\")\n    print(f\"Average Validation Accuracy: {result['avg_validation_accuracy']:.4f}, Average Validation Loss: {result['avg_validation_loss']:.4f}\")\n    print(\"=\"*50)\n\n# Optionally, sort results by average validation accuracy\nsorted_results = sorted(results, key=lambda x: x[\"avg_validation_accuracy\"], reverse=True)\n\n# Print sorted best hyperparameters\nprint(\"Best hyperparameters based on average validation accuracy:\")\nfor result in sorted_results[:5]:  # Top 5 results\n    print(f\"{result['num_trees']} trees, max_depth={result['max_depth']}, shrinkage={result['shrinkage']}, subsample={result['subsample']}\")\n    print(f\"Average Validation Accuracy: {result['avg_validation_accuracy']:.4f}\")\n    print(\"-\"*50)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=\"Transported\")\nvalid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label=\"Transported\")\nrf = RandomForestClassifier(n_estimators=200, max_depth=8)\nxgb = XGBClassifier(n_estimators=300, learning_rate=0.05)\nlr = LogisticRegression()\n\nensemble_model = VotingClassifier(estimators=[\n    ('rf', rf),\n    ('xgb', xgb),\n    ('lr', lr)\n], voting='soft')\n\nensemble_model.fit(train_ds_pd.drop(columns=[\"Transported\"]), train_ds_pd[\"Transported\"])","metadata":{}},{"cell_type":"code","source":"print(f\"Available variable importances:\")\nfor importance in inspector.variable_importances().keys():\n  print(\"\\t\", importance)\ninspector.variable_importances()[\"NUM_AS_ROOT\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#tfdf.model_plotter.plot_model_in_colab(rf, tree_idx=0, max_depth=3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow_decision_forests as tfdf\n\n# Load the test dataset\ntest_df = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')\nsubmission_id = test_df[\"PassengerId\"]\n\n# Fill missing values\n\ntest_df[['VIP', 'CryoSleep']] = test_df[['VIP', 'CryoSleep']].fillna(value=0)\n\n# Create new features \ntest_df['FamilySize'] = test_df['PassengerId'].apply(lambda x: int(x.split('_')[1]))\ntest_df[[\"Deck\", \"Cabin_num\", \"Side\"]] = test_df[\"Cabin\"].str.split(\"/\", expand=True)\ntest_df.drop(columns=['Cabin'], inplace=True)\n\n# Age groups\nbins = [0, 5, 10, 18, 25, 35, 45, 60, 100]\nlabels = ['0-5', '6-10', '11-18', '19-25', '26-35', '36-45', '46-60', '60+']\n\ntest_df['Age_Group'] = pd.cut(test_df['Age'], bins=bins, labels=labels, right=False)\n\ndrop_features = [\"Age\",'Name','PassengerId']\n\ntest_df['Cabin_num'] = pd.to_numeric(test_df['Cabin_num'], errors='coerce')\n\ntest_df[\"TotalSpending\"] = (\n    test_df[\"Spa\"] + test_df[\"VRDeck\"] + \n    test_df[\"RoomService\"] + test_df[\"ShoppingMall\"] + \n    test_df[\"FoodCourt\"]\n)\n\ntest_df[\"LuxuryUser\"] = (test_df[\"Spa\"] > 0) | (test_df[\"VRDeck\"] > 0) | (test_df[\"RoomService\"] > 0)\ntest_df[\"SpendingPerAge\"] = test_df[\"TotalSpending\"] / (test_df[\"Age\"] + 1)\n\ntest_df[\"HighSpender\"] = test_df[\"TotalSpending\"] > test_df[\"TotalSpending\"].median()\ntest_df = pd.get_dummies(test_df, columns=['Age_Group'], drop_first=False)\n\ntest_df = test_df.drop(columns=drop_features)\n# Convert DataFrame to TensorFlow dataset\ntest_df = test_df.map(lambda x: int(x) if isinstance(x,bool) else x)\ntest_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df)\n\n\n# Get predictions for test data\npredictions = gbm.predict(test_ds)\nn_predictions = (predictions > 0.5).astype(bool)\n\n# Create submission file\noutput = pd.DataFrame({'PassengerId': submission_id,\n                       'Transported': n_predictions.squeeze()})\n\noutput.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output.to_csv('/kaggle/working/submission.csv', index=False)\noutput.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.isnull().sum().sort_values(ascending=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# NOTES\n\nTried things that got worse result:\n\n- Age groups groupped by planet \n- Keeping age column\n- Dropping rows with NaN\n- Filling all rows with 0\n- Filling test data similiarly to training data\n","metadata":{"execution":{"iopub.status.busy":"2025-03-30T17:38:13.984564Z","iopub.execute_input":"2025-03-30T17:38:13.984921Z","iopub.status.idle":"2025-03-30T17:38:14.014660Z","shell.execute_reply.started":"2025-03-30T17:38:13.984882Z","shell.execute_reply":"2025-03-30T17:38:14.012699Z"}}}]}