{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":124833,"databundleVersionId":14702338,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-05T22:23:56.513231Z","iopub.execute_input":"2025-12-05T22:23:56.513575Z","iopub.status.idle":"2025-12-05T22:23:56.523754Z","shell.execute_reply.started":"2025-12-05T22:23:56.513550Z","shell.execute_reply":"2025-12-05T22:23:56.522696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train  = pd.read_csv(\"/kaggle/input/binary-classification-of-insurance-cross-selling/train.csv\")\ntest  = pd.read_csv(\"/kaggle/input/binary-classification-of-insurance-cross-selling/test.csv\")\ntrain = train.drop(columns='id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T22:23:56.525449Z","iopub.execute_input":"2025-12-05T22:23:56.525738Z","iopub.status.idle":"2025-12-05T22:24:24.723536Z","shell.execute_reply.started":"2025-12-05T22:23:56.525718Z","shell.execute_reply":"2025-12-05T22:24:24.722465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" print(train.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T22:24:24.724568Z","iopub.execute_input":"2025-12-05T22:24:24.724980Z","iopub.status.idle":"2025-12-05T22:24:24.737000Z","shell.execute_reply.started":"2025-12-05T22:24:24.724948Z","shell.execute_reply":"2025-12-05T22:24:24.735881Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_features = train.select_dtypes(include=[\"object\"]).columns.tolist()\nnumerical_features = train.select_dtypes(exclude=[\"object\"]).columns.tolist()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T22:24:24.739351Z","iopub.execute_input":"2025-12-05T22:24:24.739685Z","iopub.status.idle":"2025-12-05T22:24:25.377070Z","shell.execute_reply.started":"2025-12-05T22:24:24.739663Z","shell.execute_reply":"2025-12-05T22:24:25.376118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in categorical_features:\n    plt.figure()\n    train[col].value_counts().plot(kind=\"bar\")\n    plt.title(f\"Count Plot of {col}\")\n    plt.xlabel(col)\n    plt.ylabel(\"Count\")\n    plt.show()\nfor col in numerical_features:\n    plt.figure()\n    plt.hist(train[col], bins=30)\n    plt.title(f\"Distribution of {col}\")\n    plt.xlabel(col)\n    plt.ylabel(\"Frequency\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T22:24:25.378312Z","iopub.execute_input":"2025-12-05T22:24:25.378628Z","iopub.status.idle":"2025-12-05T22:24:32.620250Z","shell.execute_reply.started":"2025-12-05T22:24:25.378582Z","shell.execute_reply":"2025-12-05T22:24:32.619321Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel_encoders = {}\n\nfor col in categorical_features:\n    le = LabelEncoder()\n    train[col] = le.fit_transform(train[col])\n    label_encoders[col] = le","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T22:24:32.621170Z","iopub.execute_input":"2025-12-05T22:24:32.621483Z","iopub.status.idle":"2025-12-05T22:24:39.496151Z","shell.execute_reply.started":"2025-12-05T22:24:32.621461Z","shell.execute_reply":"2025-12-05T22:24:39.495416Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corr_matrix = train.corr()\nresponse_corr = corr_matrix[\"Response\"].sort_values(ascending=False)\n#print(response_corr)\nplt.figure(figsize=(12, 9))\nsns.heatmap(\n    corr_matrix,\n    annot=True,\n    fmt=\".2f\",\n    linewidths=0.5\n)\n\nplt.title(\"Correlation Heatmap\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T22:24:39.497182Z","iopub.execute_input":"2025-12-05T22:24:39.497859Z","iopub.status.idle":"2025-12-05T22:24:45.165801Z","shell.execute_reply.started":"2025-12-05T22:24:39.497829Z","shell.execute_reply":"2025-12-05T22:24:45.164898Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(response_corr)\nprint(corr_matrix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T22:24:45.168044Z","iopub.execute_input":"2025-12-05T22:24:45.168390Z","iopub.status.idle":"2025-12-05T22:24:45.180249Z","shell.execute_reply.started":"2025-12-05T22:24:45.168367Z","shell.execute_reply":"2025-12-05T22:24:45.179298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_engineered_features(df):\n    \"\"\"\n    Create new features based on correlation analysis.\n    \n    Key Insights:\n    - Previously_Insured & Vehicle_Damage: -0.84 correlation (highly complementary)\n    - Age correlates with Vehicle_Age (-0.54), Policy_Sales_Channel (-0.59)\n    - Response strongest with: Vehicle_Damage (0.36), Previously_Insured (-0.35)\n    \"\"\"\n    \n    df_new = df.copy()\n    \n    print(\"\\nüìä Creating Engineered Features...\")\n    \n    # Feature 1: Risk Profile Score\n    # Combines the two strongest predictors with opposite effects\n    # Not insured (1) + Vehicle damaged (1) = High interest (score=3)\n    df_new['risk_profile'] = (1 - df['Previously_Insured']) * 2 + df['Vehicle_Damage']\n    print(\"‚úì Created 'risk_profile' - Combines insurance status & vehicle damage\")\n    \n    # Feature 2: New Customer with Damage (Interaction)\n    # This captures the sweet spot: customers who need insurance AND have damage\n    df_new['new_customer_damage'] = (1 - df['Previously_Insured']) * df['Vehicle_Damage']\n    print(\"‚úì Created 'new_customer_damage' - Key interaction term\")\n    \n    # Feature 3: Age-Vehicle Mismatch\n    # Unusual combinations indicate different insurance needs\n    age_norm = (df['Age'] - df['Age'].mean()) / df['Age'].std()\n    vehicle_age_norm = (df['Vehicle_Age'] - df['Vehicle_Age'].mean()) / df['Vehicle_Age'].std()\n    df_new['age_vehicle_mismatch'] = np.abs(age_norm + vehicle_age_norm)\n    print(\"‚úì Created 'age_vehicle_mismatch' - Captures unusual age-vehicle combos\")\n    \n    # Feature 4: Customer Maturity Index\n    # Sophisticated customers behave differently\n    df_new['customer_maturity'] = (\n        (df['Age'] / 100) * 0.5 +\n        df['Previously_Insured'] * 0.3 +\n        (df['Policy_Sales_Channel'] / df['Policy_Sales_Channel'].max()) * 0.2\n    )\n    print(\"‚úì Created 'customer_maturity' - Overall customer sophistication score\")\n    \n    # Feature 5: Premium Affordability by Age\n    # Context matters - same premium means different things at different ages\n    df_new['premium_age_ratio'] = df['Annual_Premium'] / (df['Age'] + 1)\n    print(\"‚úì Created 'premium_age_ratio' - Premium affordability metric\")\n    \n    # Feature 6: Channel-Age Interaction\n    # Different channels work better for different age groups (-0.59 correlation)\n    df_new['channel_age_interaction'] = df['Policy_Sales_Channel'] * (100 - df['Age']) / 100\n    print(\"‚úì Created 'channel_age_interaction' - Age-specific channel effectiveness\")\n    \n    # Feature 7: Vehicle Risk Score\n    # Old damaged vehicles have different insurance appeal\n    df_new['vehicle_risk_score'] = df['Vehicle_Age'] * 0.5 + df['Vehicle_Damage'] * 0.5\n    print(\"‚úì Created 'vehicle_risk_score' - Combined vehicle risk indicator\")\n    \n    # Feature 8: Premium Vintage Ratio\n    # How premium relates to customer tenure\n    df_new['premium_vintage_ratio'] = df['Annual_Premium'] / (df['Vintage'] + 1)\n    print(\"‚úì Created 'premium_vintage_ratio' - Premium per tenure period\")\n    \n    # Feature 9: High-Value Prospect Flag\n    # Binary flag for ideal customer profile\n    df_new['high_value_prospect'] = (\n        (df['Vehicle_Damage'] == 1) &\n        (df['Previously_Insured'] == 0) &\n        (df['Age'] > 30) &\n        (df['Annual_Premium'] > df['Annual_Premium'].median())\n    ).astype(int)\n    print(\"‚úì Created 'high_value_prospect' - Ideal customer flag\")\n    \n    # Feature 10: Gender-Age Interaction\n    df_new['gender_age_interaction'] = df['Gender'] * (df['Age'] / 100)\n    print(\"‚úì Created 'gender_age_interaction' - Gender effect by age\")\n    \n    print(f\"\\n‚úÖ Total features: {len(df_new.columns)} (added {len(df_new.columns) - len(df.columns)} new features)\")\n    \n    return df_new","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T22:24:45.181299Z","iopub.execute_input":"2025-12-05T22:24:45.181650Z","iopub.status.idle":"2025-12-05T22:24:45.205966Z","shell.execute_reply.started":"2025-12-05T22:24:45.181620Z","shell.execute_reply":"2025-12-05T22:24:45.204941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, roc_curve","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T22:24:45.208810Z","iopub.execute_input":"2025-12-05T22:24:45.209098Z","iopub.status.idle":"2025-12-05T22:24:45.235283Z","shell.execute_reply.started":"2025-12-05T22:24:45.209077Z","shell.execute_reply":"2025-12-05T22:24:45.233945Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = train.drop('Response', axis=1)\ny = train['Response']\n\n# Engineer features\nX = create_engineered_features(X)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T22:24:45.236331Z","iopub.execute_input":"2025-12-05T22:24:45.236672Z","iopub.status.idle":"2025-12-05T22:24:57.654257Z","shell.execute_reply.started":"2025-12-05T22:24:45.236629Z","shell.execute_reply":"2025-12-05T22:24:57.653307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_and_train_ensemble(X_train, y_train):\n    \n    # Model 1: XGBoost - Great for capturing complex patterns\n    print(\"\\nüîß Initializing XGBoost...\")\n    xgb_model = XGBClassifier(\n        n_estimators=500,\n        max_depth=12,\n        learning_rate=0.1,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        eval_metric='logloss',\n        use_label_encoder=False\n    )\n    \n    # Model 2: Random Forest - Robust and interpretable\n    print(\"üîß Initializing Random Forest...\")\n    rf_model = RandomForestClassifier(\n        n_estimators=500,\n        max_depth=6,\n        min_samples_split=20,\n        min_samples_leaf=10,\n        random_state=42,\n        n_jobs=-1\n    )\n    \n    # Ensemble: Voting Classifier with soft voting (probability averaging)\n    print(\"üîß Creating Voting Ensemble...\")\n    ensemble = VotingClassifier(\n        estimators=[\n            ('xgb', xgb_model),\n            ('rf', rf_model)\n        ],\n        voting='soft',\n        weights=[0.3 , 0.7], \n        n_jobs=-1\n    )\n    \n    # Train the ensemble\n    print(\"\\nüöÄ Training ensemble model...\")\n    ensemble.fit(X_train, y_train)\n    print(\"‚úÖ Training complete!\")\n    \n    return ensemble\ndef evaluate_model(ensemble, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Comprehensive model evaluation with visualizations.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"MODEL EVALUATION\")\n    print(\"=\" * 80)\n    \n    # Predictions\n    y_pred_train = ensemble.predict(X_train)\n    y_pred_test = ensemble.predict(X_test)\n    y_pred_proba_train = ensemble.predict_proba(X_train)[:, 1]\n    y_pred_proba_test = ensemble.predict_proba(X_test)[:, 1]\n    \n    # Performance metrics\n    print(\"\\nüìà TRAINING SET PERFORMANCE:\")\n    print(f\"ROC-AUC Score: {roc_auc_score(y_train, y_pred_proba_train):.4f}\")\n    \n    print(\"\\nüìä TESTING SET PERFORMANCE:\")\n    print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba_test):.4f}\")\n    \n    print(\"\\nüìã Classification Report (Test Set):\")\n    print(classification_report(y_test, y_pred_test))\n    \n    print(\"\\nüî¢ Confusion Matrix (Test Set):\")\n    cm = confusion_matrix(y_test, y_pred_test)\n    print(cm)\n    \n    # Visualization 1: Confusion Matrix\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n    axes[0].set_title('Confusion Matrix (Test Set)', fontsize=14, fontweight='bold')\n    axes[0].set_ylabel('True Label')\n    axes[0].set_xlabel('Predicted Label')\n    \n    # Visualization 2: ROC Curve\n    fpr, tpr, _ = roc_curve(y_test, y_pred_proba_test)\n    auc_score = roc_auc_score(y_test, y_pred_proba_test)\n    \n    axes[1].plot(fpr, tpr, label=f'Ensemble (AUC = {auc_score:.4f})', linewidth=2)\n    axes[1].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n    axes[1].set_xlabel('False Positive Rate')\n    axes[1].set_ylabel('True Positive Rate')\n    axes[1].set_title('ROC Curve', fontsize=14, fontweight='bold')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return y_pred_proba_test\n\n\ndef plot_feature_importance(ensemble, feature_names):\n    \"\"\"\n    Plot feature importance from both models.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"FEATURE IMPORTANCE ANALYSIS\")\n    print(\"=\" * 80)\n    \n    # Extract models from ensemble\n    xgb_model = ensemble.named_estimators_['xgb']\n    rf_model = ensemble.named_estimators_['rf']\n    \n    # Get feature importances\n    xgb_importance = xgb_model.feature_importances_\n    rf_importance = rf_model.feature_importances_\n    \n    # Create DataFrame\n    importance_df = pd.DataFrame({\n        'Feature': feature_names,\n        'XGBoost': xgb_importance,\n        'RandomForest': rf_importance,\n        'Average': xgb_importance * 0.6 + rf_importance * 0.4\n    }).sort_values('Average', ascending=False)\n    \n    print(\"\\nüèÜ Top 15 Most Important Features:\")\n    print(importance_df.head(15).to_string(index=False))\n    \n    # Visualization\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # Top 10 features\n    top_features = importance_df.head(10)\n    \n    # XGBoost importance\n    axes[0].barh(range(len(top_features)), top_features['XGBoost'], color='#1f77b4')\n    axes[0].set_yticks(range(len(top_features)))\n    axes[0].set_yticklabels(top_features['Feature'])\n    axes[0].invert_yaxis()\n    axes[0].set_xlabel('Importance Score')\n    axes[0].set_title('XGBoost Feature Importance (Top 10)', fontsize=12, fontweight='bold')\n    axes[0].grid(True, alpha=0.3)\n    \n    # Random Forest importance\n    axes[1].barh(range(len(top_features)), top_features['RandomForest'], color='#2ca02c')\n    axes[1].set_yticks(range(len(top_features)))\n    axes[1].set_yticklabels(top_features['Feature'])\n    axes[1].invert_yaxis()\n    axes[1].set_xlabel('Importance Score')\n    axes[1].set_title('Random Forest Feature Importance (Top 10)', fontsize=12, fontweight='bold')\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return importance_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T22:24:57.655219Z","iopub.execute_input":"2025-12-05T22:24:57.655537Z","iopub.status.idle":"2025-12-05T22:24:57.678955Z","shell.execute_reply.started":"2025-12-05T22:24:57.655507Z","shell.execute_reply":"2025-12-05T22:24:57.677792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ensemble_model = build_and_train_ensemble(X_train, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T22:24:57.679975Z","iopub.execute_input":"2025-12-05T22:24:57.680321Z","iopub.status.idle":"2025-12-05T23:10:16.529765Z","shell.execute_reply.started":"2025-12-05T22:24:57.680292Z","shell.execute_reply":"2025-12-05T23:10:16.527923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_proba_test = evaluate_model(ensemble_model, X_train, X_test, y_train, y_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T01:31:20.936861Z","iopub.execute_input":"2025-12-06T01:31:20.937071Z","iopub.status.idle":"2025-12-06T01:31:21.014794Z","shell.execute_reply.started":"2025-12-06T01:31:20.937051Z","shell.execute_reply":"2025-12-06T01:31:21.013598Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_names = X.columns.tolist()\nimportance_df = plot_feature_importance(ensemble_model, feature_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T23:24:28.027845Z","iopub.execute_input":"2025-12-05T23:24:28.028467Z","iopub.status.idle":"2025-12-05T23:24:28.075550Z","shell.execute_reply.started":"2025-12-05T23:24:28.028442Z","shell.execute_reply":"2025-12-05T23:24:28.074018Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}